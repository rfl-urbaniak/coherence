---
title: "Measuring coherence with Bayesian Networks"
# author: 
#   - "\\normalsize Rafal Urbaniak ([LoPSE research group](http://lopsegdansk.blogspot.com/p/lopse-team.html), University of Gdansk)"
#   - "\\normalsize Alicja Kowalewska  (Carnegie Mellon University \\& [LoPSE research group](http://lopsegdansk.blogspot.com/p/lopse-team.html))" 
output:
  pdf_document:
    number_sections: true
    df_print: kable 
    keep_tex: true
    toc_depth: 3
    includes:
      in_header:
        - Rafal_latex4.sty
fontsize: 10pt
bibliography: munich3.bib
csl: apa-single-spaced.csl  
documentclass: scrartcl
linkcolor: blue 
filecolor: blue
citecolor: blue
urlcolor: blue
toccolor: blue
---

```{r setup, include=FALSE, echo=FALSE}
require("knitr")
opts_knit$set(root.dir = "../../")

```


```{r setup2, include=FALSE, cache = TRUE}
library(tidyverse)
library(ggplot2)
library(gridExtra)
library(bnlearn)
library(knitr)
library(kableExtra)
library(gRain)
library(reshape2)
library(tidyverse)
library(plyr)
library(rje)
library(bnlearn)
library(utils)
library(latex2exp)
library(useful)
library(tidyverse)
library(stringr)
library(plot3D)
library(tinytex)


source("utils//CombinationsBN.R")
source("utils//CptCreate.R")
source("utils//LogicAndBNs.R")
source("utils//kableCPTs.R")
```



\begin{abstract} \textbf{Abstract.}

The notion of coherence is often used in many philosophical, especially epistemological, discussions (for instance, in discussions  about the  truth-conduciveness of coherence). An explication of the key notion involved seems desirable.  We introduce the most prominent coherence measures and a number of counterexamples put forward against them.  Then, we point out some common problems that underlie these counterexamples. These observations lead  us to a new measure of coherence. Our measure  diverges from the known candidates in three important respects: (1) It is not a function of a probabilistic measure and a set of propositions alone, because it is also sensitive to the selection and direction of arrows in a Bayesian Network representing an agent's credal state. (2) Unlike in the case of quite a few coherence measures, it is not obtained by taking a mean of some list of intermediate values (such as confirmation levels between subsets of a narration). It  is sensitive also to the variance and the minimal values of the intermediate values. (3) The intermediate values used are not confirmation levels, but rather expected and weighted confirmation levels. We apply our measure to the existing counterexamples and compare its performance to the performance of the other measures. It does a  better job. 
\end{abstract}

\section{Motivations \& introduction}

The notion of coherence is often used in many philosophical, especially epistemological, discussions (for instance, in discussions  about the  truth-conduciveness of coherence). An explication of the key notion involved seems desirable. 


There is also a  more practical  reason to develop a better understanding of the notion: a plausible measure of coherence could be used to better evaluate the quality of some stories or narrations. For example in the legal context we would like to be able to assess the quality of a testimony in the court of law. Focusing only on the probability of a story is to some extent problematic, because from such a perspective, more detailed stories are  penalized --- they contain more propositions, so they (usually) have lower probabilities. A plausible coherence measure could be used to asses an important aspect of a narration which so far seems to escape probabilistic analysis. 
 


When we talk about the coherence of a set of propositions or about the coherence of a story, we seem to  refer to   how well their individual pieces fit together.  How are we to understand and apply this notion systematically, though?

As with beliefs, we can use both a binary and a graded notion of coherence. The binary notion is not very exciting: a set is incoherent just in case it is logically inconsistent.\footnote{There is a related notion in the neighborhood where an agent's  degrees of beliefs are coherent just in case it they are probabilistic. We will not use this notion in this paper.}    Intuitively, graded coherence should be a generalization of this requirement: logically incoherent sets should have minimal (or at least negative) level of graded coherence. Or, at least, lower coherence than consistent ones. What other requirements should a coherence measure satisfy and how should it be explicated formally, if we want to massage this notion into a more general framework of probabilistic epistemology? 
Defining a measure of graded coherence in probabilistic terms  turned out to  be quite a challenge, which resulted in heaps of literature. 



Also, in reasarch unconnected to  and seemingly unaware of the philosophical discussion, in the context of Bayesian networks developed for stories and narrations in legal contexts [@vlek2013modeling, @vlek2014building, @vlek2015, @vlek2016stories, @fenton2013GeneralStructureLegal, @fenton2013GeneralStructureLegal], an approach to coherence has been developed by Vlek. The proposal is to capture the coherence of the story by introducing a single narration root node which becomes an ancestor  node to all the other nodes such that the conditional probability of each dependent node given that the state of this root is 1 (that is, the corresponding proposition is assumed to be true), is also 1. This is defended by observing that in such a network previously independent  nodes become dependent without any principled reason. This is true, but we don't think this is desirable: one shouldn't introduce probabilistic dependencies by fiat in a model. Moreover, Vlek then identifies coherence of a model with the prior probability of the narration node, and we specifically want to capture the idea that coherence is distinct from probability. For this reason, we think an account of coherence for such practical applications is still missing. 



We first introduce the main existing coherence measures. Then we describe a lengthy list of counterexamples to these measures. A general discussion of certain common features and issues we observed follows, which leads us to the description of our own coherence measure.

Our measure  diverges from the known candidates in three important respects: (1) It is not a function of a probabilistic measure and a set of propositions alone, because it is also sensitive to the selection and direction of arrows in a Bayesian Network representing an agent's credal state. (2) Unlike in the case of quite a few coherence measures, it is not obtained by taking a mean of some list of intermediate values (such as confirmation levels between subsets of a narration). It  is sensitive also to the variance and the minimal values of the intermediate values. (3) The intermediate values used are not confirmation levels, but rather expected and weighted confirmation levels (read on for details). Finally, we apply our measure to the existing counterexamples and compare its performance to the performance of the other measures. Spoiler alert: it does a  better job.


The whole work has been made possible by all those who contributed to the development of \textsf{\textbf{R}} language, and Marco Scutari, the author of \textsf{\textbf{bnlearn}} package, who was kind enough to extend his package with additional features upon our requests. The use of these tools here is essential, because we used the environment to write Bayesian Networks (BNs) for all the counterexamples, all coherence functions as applicable to BNs (including ours), and automated  performance analysis and BN visualisation, which otherwise wouldn't  be manageable.\footnote{Our code can be found at: TODO-repo-link.}









\section{Measures}
Let’s take a look at different approaches to measuring coherence. One thing to keep in mind is that different measures use different scales and have different neutral points, if any  (the idea is: the coherence of probabilistically independent propositions should be neither positive nor negative).



\subsection{Deviation from independence -- Shogenji}
The first measure we present was developed by @shogenji1999[340]  and is often called \textit{deviation from independence}. This measure is defined as the ratio between the  probability of the conjunction of all claims, and the probability that the conjunction  would get if all its conjuncts were probabilistically independent. 
\begin{align}
    \tag{Shogenji}
    \label{coh:Shogenji}
    C_s(A_1,\dots,A_n)=\frac{P(A_1 \& \dots \& A_n)}{P(A_1)\times\dots\times P(A_n)}
\end{align}

\noindent \textbf{scale:} [0, $\infty$] 

\noindent  \textbf{neutral point:} 1 

\noindent This measure was later generalized by @meijs2007. According to this approach, \eqref{coh:Shogenji} is applied not only to the whole set of propositions, but to each non-empty non-singleton subset of the set, and  the final value is defined as the average of all sub-values thus obtained.



\subsection{Relative overlap -- Olsson \& Glass}

 The second approach, a \textit{relative overlap} measure, comes from @olsson2001 and @glass2002. This measure is defined as the ratio between the intersection of all propositions and their union. It was later generalized in a way analogous to the one used in the generalization of  the Shogenji's measure.

\begin{align}
    \tag{Olsson}
    \label{coh:Olsson}
    C_o(A_1,\dots,A_n)=\frac{P(A_1 \& \dots \& A_n)}{P(A_1 \vee \dots \vee A_n)}
\end{align}

\noindent \textbf{scale:} [0, 1] 

\noindent \textbf{neutral point:} NO 


\subsection{Average mutual support}

Finally, the most recent approach --- a class of measures called \textit{average mutual support}. The general recipe for such a measure is this.

\begin{itemize}
\item
  Given that \(S\) is a set whose coherence is to be measured, let \(P\)
  indicate the set of all ordered pairs of non-empty, disjoint subsets
  of \(S\).
\item
  First, define a confirmation measure for the confirmation of a hypothesis \(H\) by evidence  \(E\): \(Conf(H,E)\).
\item
  For each pair \(\langle X, Y \rangle \in P\), calculate
  \(Conf(\bigwedge X, \bigwedge Y)\), where $\bigwedge X$ ($\bigwedge Y$) is the conjunction of all the elements of $X$ ($Y$).
\item
  Take the mean of all the results.
\end{itemize}
\begin{align*}
    \mathcal{C}(P) & =
mean\left(\left\{Conf(\bigwedge X_i, \bigwedge Y_i) | \langle X_i, Y_i \rangle \in P\right\} \right)
\end{align*}

\noindent Depending on the choice of a confirmation measure, we achieve different measures of coherence. 


\subsubsection{Fitelson}

@fitelson2003ProbabilisticTheoryCoherence[@fitelson2003comments] uses the following confirmation function:

\begin{align*}
    F(H,E) = \begin{cases}
    1 & E\models H, E\not \models \bot \\
    -1 & E \models \neg H\\
    \frac{P(E|H)-P(E|\neg H)}{P(E|H)+P(E|\neg H)} & \mbox{o/w}
    \end{cases}
\end{align*}

\begin{align}
\tag{Fitelson}  
    \mathcal{C}_{F}(P) & =
mean\left(\left\{F(\bigwedge X_i, \bigwedge Y_i) | \langle X_i, Y_i\rangle \in P\right\} \right)
\end{align}

\noindent For instance, Fitelson's coherence for two propositions boils down to this:
\begin{align}    
    \tag{Fitelson pairs}  
    \mathcal{C}_{F}(X,Y) &= \frac{F(X,Y)+F(Y,X)}{2}
    \label{coh:Fitelson}
\end{align}


\noindent \textbf{scale:} [-1, 1] 

\noindent \textbf{neutral point:} 0 



\subsubsection{Douven and Meij's}

Another coherence measure of this type has been introduced by @meijs2007[412] 

They use the  \textit{difference} confirmation measure:
\begin{align*}
    D(H,E) = P(H|E) - P(H)
\end{align*}
The resulting definition of coherence is:
\begin{align}
\tag{DM}  
    \mathcal{C}_{DM}(P) & =
mean\left(\left\{D(\bigwedge X_i, \bigwedge Y_i) | \langle X_i, Y_i\rangle \in P\right\} \right)
\end{align}


For two propositions, the coherence measure boils down to:
\begin{align}
    \tag{DM pairs}
    \label{coh:DM}
    C_{DM}(X,Y)= \frac{P(X|Y) - P(X) + P(Y|X) - P(Y)}{2}
\end{align}


\noindent \textbf{scale:} [-1, 1] 


\noindent \textbf{neutral point:} 0 (not explicit) 



\subsubsection{Roche}

Yet another measure, due to  @Roche2013Coherence[69],  starts with  the  absolute confirmation measure:
\begin{align*}
    A(H,E) = \begin{cases}
    1 & E\models H, E\not \models \bot \\
    0 & E \models \neg H\\
    P(H|E) & \mbox{o/w} \\
    \end{cases}
\end{align*}
which results in the following coherence measure:
\begin{align}
\tag{Roche}  
    \mathcal{C}_{R}(P) & =
mean\left(\left\{A(\bigwedge X_i, \bigwedge Y_i) | \langle X_i, Y_i\rangle \in P\right\} \right) 
\end{align}

For two propositions, the measure gives the following:
\begin{align}
    \tag{Roche pairs}
    \label{coh:Roche}
     C_{R}(X,Y)= \frac{P(X|Y)+P(Y|X)}{2}
\end{align}

\noindent \textbf{scale:} [0, 1] 


\noindent \textbf{neutral point:} 0.5 








\section{Challenges}


Here are the counterexamples put forward against various coherence measures in the literature. We ignored only a few  where both we didn't share the authors' intuitions and the examples were not picked up in further discussion in the literature. 

\subsection{Penguins} 

\textbf{The scenario.}  A challenge discussed in [@bovens2004bayesian,50] and [@meijs2007] consists of the following set of propositions (instead of \emph{letters} or \emph{abbreviations}, we'll talk about \emph{nodes}, as these will be used later on in Bayesian networks):


```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
source("bns//Dunnit.R")
B <- "Tweety is a bird."
G <-  "Tweety is a grounded animal."
P <-   "Tweety is a penguin."


penguin <- data.frame(c("B","G","P"),c(B,G,P))
colnames(penguin) <- c("node","content")
penguin %>%  kable(format = "latex",booktabs=T,
                        linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped")) 
```




\noindent \textbf{Desiderata.}
It seems that the set \{\s{B},\s{G}\}, which doesn't contain the information about Tweety being a penguin, should be less coherent than the one that does contain this information: \{\s{B},\s{G},\s{P}\}.

\vspace{2mm}\begin{description}
    \item[(\s{BG}$<$\s{BGP})] \{\s{B},\s{G}\}  should be less coherent than \{\s{B},\s{G},\s{P}\}. 
\end{description}\vspace{2mm}

Another intuition about this scenario [@Schippers2019General] is that when you consider a set which says that Tweety is both a bird and a penguin: \{\s{B},\s{P}\}, adding proposition about not flying (\s{G}) shouldn't really increase the coherence of the set.  It's a well-known fact that penguins don’t fly, and so one can deduce \s{G} from \s{P}. Therefore by adding \s{G} explicitly to the set, one wouldn't gain any new information -- so if a set expresses the same information, its coherence shouldn't be different. 
However, as \s{G} is not a logical consequence of \s{P}, it can be argued that \{\s{B},\s{P}\} and \{\s{B},\s{P},\s{G}\} represent different information sets, and a slight difference in their coherence is also acceptable.

\vspace{2mm}\begin{description}
    \item[(\s{BP}$\approx$\s{BGP})]  \{\s{B},\s{P}\} should have similar coherence to \{\s{B},\s{P},\s{G}\}.  
\end{description}\vspace{2mm}

<!--  \todo{do something about this remark} -->
<!-- \textbf{Remarks} -->
<!-- \begin{itemize} -->
<!--     \item This scenario might suggest that we don't want any-any coherence measure. B \& G are in tension, but by adding P we explain this tension, so why does it still count against overall coherence? Similarly, G doesn't really support B or P, but no one claims that Twitty is a penguin BECAUSE it is grounded, or that it is a bird BECAUSE it is grounded. The causal relationship seems obvious here - Twitty is grounded BECAUSE it is a penguin. -->
<!-- \end{itemize} -->

\subsection{Dunnit}



\textbf{The scenario.} Another challenge, introduced by  @Merricks1995 goes as follows:  Mr. Dunnit is a suspect in the murder case. Detectives first obtained the following body of evidence:   


```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
source("bns//Dunnit.R")
I <- "Witnesses claim to have seen Dunnit do it (incriminating testimony)."
M <-  "Dunnit had a motive for the murder."
W <-   "A credible witness claims to have seen Dunnit two hundred miles from the scene of the crime at the time of the murder."


dunnit <- data.frame(c("I","M","W"),c(I,M,W))
colnames(dunnit) <- c("node","content")
dunnit %>%  kable(format = "latex",booktabs=T,
                        linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped")) %>% column_spec(2, width = "25em")
```


In light of this information they try to assess whether Dunnit is responsible for the crime.


```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
source("bns//Dunnit.R")
G <-  "Dunnit is guilty."

dunnit <- data.frame(c("G"),c(G))
colnames(dunnit) <- c("node","content")
dunnit %>%  kable(format = "latex",booktabs=T,
                        linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped")) %>% column_spec(2, width = "25em")
```

Now, suppose the detectives learn Dunnit has a twin brother. 

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
source("bns//Dunnit.R")
Tw <- "Dunnit has an identical twin which was seen by the credible witness two hundred miles from the scene of the crime during the murder."

dunnit <- data.frame(c("Tw"),c(Tw))
colnames(dunnit) <- c("node","content")
dunnit %>%  kable(format = "latex",booktabs=T,
                        linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped")) %>% column_spec(2, width = "25em")
```





\noindent and compare the coherence of $\{$\s{I,M,W,G}$\}$ with the coherence of   $\{$\s{I,M,W,G,Tw}$\}$. 






\noindent \textbf{Desideratum.}  It seems that adding proposition about a twin should increase the coherence of the set.

\vspace{2mm}\begin{description}
    \item[(Dunnit$<$Twin)] $\{$\s{I,M,W,G}$\}$ should be less coherent than $\{$\s{I,M,W,G,Tw}$\}$. 
\end{description}\vspace{2mm}



\subsection{Japanese  swords}

\textbf{The scenario.} The next challenge comes from   [@meijs2007,414]:

\begin{quote}
  We start by considering two situations in both of which it is assumed that a murder has been committed in a street in a big city with 10,000,000 inhabitants, 1,059 of them being Japanese, 1,059 of them owning Samurai swords, and 9 of them both being Japanese and owning Samurai swords. In situation I we assume that the murderer lives in the city and that everyone living in the city is equally likely to be the murderer. In situation II, on the other hand, we make the assumption that the victim was murdered by someone living in the street in which her body was found. In that street live 100 persons, 10 of them being Japanese, 10 owning a Samurai sword, and 9 both being Japanese and owning a Samurai sword. [\dots] [In situation III] we have 12 suspects who all live in the same house, and 10 of them are Japanese, 10 own a Samurai sword, and 9 are both Japanese and Samurai sword owners.
\end{quote}

 
The nodes involved are as follows: 


```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
source("bns//JapaneseSwords.R")
J <- "The murderer is Japanese."
O <-  "The murderer owns a Samurai sword."

swords <- data.frame(c("J","O"),c(J,O))
colnames(swords) <- c("node","content")
swords %>%  kable(format = "latex",booktabs=T,
                        linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped"))
```

And we look at three separate scenarios: (\textsf{1})      The murderer lives in the city, (\textsf{2})   The murderer lives in the street popular amongst Japanese owners of Samurai swords, and (\textsf{3})      The murderer lives in the house with many Japanese owners of Samurai swords.






<!-- % That our intuition is seemingly based on considerations of relative overlap in this case does not mean that we would endorse the general claim that such considerations are to decide the matter whenever the measures of confirmation fail to provide a unanimous answer to the question whether the given propositions support one another more in one situation than in another. Suppose, for instance, we have 12 suspects who all live in the same house, and that 10 of them are Japanese, 10 own a Samurai sword, and 9 are both Japanese and Samurai sword owners. One can verify that here, too, the measures of confirmation do not answer unanimously the question whether J and O support one another more or less in this third situation than they do in situation I of the example in the text. So, if relative-overlap considerations were to be the arbiter in this kind of case, then, since the relative overlap of J and O is the same in situation III as it is in situation II, we would be compelled to say that in situation III {J,O} is more coherent than it is in situation I. Yet we do not feel that relative-overlap considerations have the same force in the new example as they have in the example in the text. Rather our intuition says that in situation III {J,O} is not very coherent and that it is not crystal-clear which of situations I and III makes {J,O} more coherent. (Our intuition that in situation III {J,O} is not very coherent may be due to the fact that in situ- ation III J and O are almost independent; if, for example, the one person that is now assumed to be a non-Japanese non-owner of a Samurai sword were to own a Samurai sword as well, then the propositions would even be negatively related.) -->



\noindent \textbf{Desiderata.} In all of the above situations  the number of Japanese owners of Samurai swords remains the same. However, situations 1 and 2 differ in the relative overlap of \s{J} and \s{O}. Because \s{J} and \s{O} are more correlated in situation 2, it seems more coherent than situation 1.

\vspace{2mm}\begin{description}
    \item[(\s{JO2}$>$\s{JO1})]  \{\s{J,O,2}\} should be more coherent than \{\s{J,O,1}\}
\end{description}\vspace{2mm}

However, bigger overlap, supposedly, doesn't have  to indicate higher coherence. In situation 3 \s{J} and \s{O} confirm each other to a lesser extent  than in situation 2 (compare $P(J|O)-P(J)$ and $P(O|J)-P(O)$ in both cases), and for this reason  Douven and Meijs  claim that situation 2 is more coherent than situation 3.
\vspace{2mm}\begin{description}
    \item[(\s{JO2}$>$\s{JO3})]  \{\s{J,O,2}\} should be more coherent than \{\s{J,O,3}\}
\end{description}\vspace{2mm}

\noindent We  don't have clear intuitions about this desideratum. It seems to be in tension with the requirement offered by @Siebel2004On-Fitelsons-me[336]  which we'll discuss in the next subsection. 



\subsection{Robbers}

\textbf{The scenario.} A challenge put forward by @Siebel2004On-Fitelsons-me[336]  goes as follows:
\begin{quote}
    Let there be ten equiprobable suspects for a murder. All of them previously committed at least one crime, two a robbery, two pickpocketing, and the remaining six both crimes. There is thus a substantial overlap: of the total of eight suspects who committed a robbery, six were also involved in pickpocketing, and conversely. 
\end{quote}



```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
W <-  "Real perpetrator status (three possible states)."
P <-  "The murderer is a pickpocket."
R <-  "The murderer is a robber."

robbers <- data.frame(c("W","P","R"),c(W,P,R))
colnames(robbers) <- c("node","content")
robbers %>%  kable(format = "latex",booktabs=T,
                        linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped"))
```









\noindent  \textbf{Desiderata.}  The first observation is that the set of propositions that corresponds to the situation in which a murderer committed both crimes should be regarded coherent. Most suspects committed both crimes, so this option is even the most probable one.
\vspace{2mm}\begin{description}
    \item[(\s{PR}\textgreater \s{neutral})] \{\s{P,R}\} should be regarded coherent. 
\end{description}\vspace{2mm}

According to @Siebel2004On-Fitelsons-me[336]  committing both crimes by the murderer should also be regarded more coherent than committing only one crime. 
\vspace{2mm}\begin{description}
    \item[(\s{PR}$>$\s{P}$\neg$\s{R})] \{\s{P,R}\} should be more coherent than \{\s{P},$\neg$\s{R}\} and \{$\neg$\s{P},\s{R}\}.
\end{description}\vspace{2mm}
\noindent  This requirement is slightly more controversial. Even though \{\s{P,R}\} is the most probable setup, \s{P} and \s{R} disconfirm each other ($Pr(P|R)<Pr(P)$ and $Pr(R|P)<Pr(R)$). Moreover, the intuition behind this desideratum seems to conflict with the intuition behind (\s{JO2}$>$\s{JO3}).  



\subsection{The Beatles}

\textbf{The scenario.} The challenge has been offered by @shogenji1999[339] to criticize defining coherence in terms of  pairwise coherence --- it shows there are jointly incoherent pairwise coherent sets. The scenario consists of the following claims: 



```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
D <-  "Exactly one of the Beatles (John, Paul, George and Ringo) is dead."
J <- "John is alive."
P <- "Paul is alive."
G <- "George is alive."
R <-  "Ringo is alive."

beatles <- data.frame(c("D","J","P","G","R"),c(D,J,P,G,R))
colnames(beatles) <- c("node","content")
beatles %>%  kable(format = "latex",booktabs=T,
                        linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped"))
```






\noindent  \textbf{Desiderata.} The set consisting of all of these propositions is logically inconsistent (even though the propositions are pairwise consistent), so it seems quite intuitive that it should be incoherent. 
\vspace{2mm}\begin{description}
    \item[(below neutral)] \{\s{D,J,P,G,R}\} should be incoherent.
\end{description}\vspace{2mm}
 We can make this desideratum a bit stronger by requiring that the coherence score for \{\s{D,J,P,G,R}\} should be minimal.
\vspace{2mm}\begin{description}
    \item[(minimal)] \{\s{D,J,P,G,R}\} should get the lowest possible coherence value.
\end{description}\vspace{2mm}
\noindent  One may argue that some coherence measures also measure the degree of incoherence, therefore logically inconsistent sets don't need to get the minimal score. We'll discuss this issue further in Section \ref{sec:mean}.



\subsection{Alicja and books}

 Prima facie, at least some sets with low posterior probability can be quite coherent, and at least some   sets with fairly high posterior probability can have low coherence.  To keep track of how various measures perform with respect to this intuition, we developed the following example. 


\noindent \textbf{The scenario.}   Alicja reads (\textsf{R}) 10\% of books she buys, but 15\% of books she buys that Rafal advised (\textsf{A}) her to read. 

Here, we just have two nodes:


```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
A <-  "Rafal adviced Alicja to read the book."
R <- "Alicja read the book."

book <- data.frame(c("A","R"),c(A,R))
colnames(book) <- c("node","content")
book %>%  kable(format = "latex",booktabs=T,
                        linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped"))
```



\noindent \textbf{Desiderata. } At least \emph{prima facie}, these conditions seem intuitive:
\vspace{2mm}\begin{description}
    \item[($\s{AR}>\s{A}\neg\s{R}$)] Given that Alicja was advised to read, it's more coherent that she read the book than not. 
    \item[($\s{AR}>\neg\s{AR}$) ] Given that Alicja read the book, it's more coherent that she was advised than not. 
    \item[($\neg\s{A}\neg\s{R}>\s{A}\neg\s{R}$) ] Given that Alicja didn't read the book, it's more coherent that she wasn't advised than that she was. 
    \item[($\neg\s{A}\neg\s{R}>\neg\s{AR}$)] Given that Alicja wasn't advised to read, it's more coherent that she didn't read the book than that she did. 
\end{description}\vspace{2mm}
    
  







\subsection{The Witnesses}

\textbf{The scenario.}This one comes from [@olsson2005,391]. Again, equally reliable witnesses try to identify a criminal. Consider the following reports (we extended the original scenario by adding \s{W5}):




The problem might be seen as involving subsets of the following nodes:

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
source("bns//Witness.R")
w1 <-  "Witness no. 1: ‘‘Steve did it’’"
w2 <-  "Witness no. 2: ‘‘Steve did it’’"
w3 <-  "Witness no. 3: ‘‘Steve, Martin or David did it’’"
w4 <- "Witness no. 4: ‘‘Steve, John or James did it’’"
w5 <- "Wittness no. 5: ‘‘Steve, John or Peter did it’’"
D  <-  "Who committed the deed (6  possible values)"

witnesses <- data.frame(c("W1","W2","W3","W4","W5","D"),c(w1,w2,w3,w4,w5,D))
colnames(witnesses) <- c("node","content")
witnesses %>%  kable(format = "latex",booktabs=T,
                        linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped"))
```

<!-- \begin{center} -->
<!-- \begin{tabular}{lp{9cm}} -->
<!-- \textsf{W1}     & Witness no. 1: ‘‘Steve did it’’  \\ -->
<!-- \textsf{W2}     & Witness no. 2: ‘‘Steve did it’’ \\ -->
<!-- \textsf{W3}    & Witness no. 3: ‘‘Steve, Martin or David did it’’ \\ -->
<!-- \textsf{W4}    & Witness no. 4: ‘‘Steve, John or James did it’’ \\ -->
<!-- \textsf{W5}    &  Witness no. 5: ‘‘Steve, John or Peter did it’’  \\ -->
<!-- \end{tabular} -->
<!-- \end{center} -->

Note that this time each proposition has the structure ‘‘Witness no. $X$ claims that \dots" instead of explicitly stating the witness' testimony. 

\textbf{Desiderata.} First, we can observe that \s{W1} and \s{W2} fully agree. Testimonies of \s{W3} and \s{W4} overlap only partially, therefore it seems that \{\s{W1},\s{W2}\} is more coherent than \{\s{W3},\s{W4}\}.
\vspace{2mm}\begin{description}
    \item[(\s{W1W2\textgreater W3W4})] \{\s{W1},\s{W2}\} should be more coherent than \{\s{W3},\s{W4}\}.
\end{description}\vspace{2mm}

Similarly, there is a greater agreement between \s{W4} and \s{W5} than \s{W3} and \s{W4}, so \{\s{W4},\s{W5}\} seems more coherent than \{\s{W3},\s{W4}\}.
\vspace{2mm}\begin{description}
    \item[(\s{W4W5\textgreater W3W4})] \{\s{W4},\s{W5}\} should be more coherent than \{\s{W3},\s{W4}\}.
\end{description}\vspace{2mm}






\subsection{Depth}

\textbf{The scenario.} There are eight equally likely suspects $1, \dots, 8$, and three equally reliable witnesses $a, b, c$, each trying to identify the person responsible for the crime. Compare two different situations -- \s{X1} and \s{X2}:
\begin{align*}
    X_1 & = \{a:(1 \vee 2 \vee 3), b:(1\vee 2 \vee 4), c:(1 \vee 3 \vee 4)\}\\
    X_2 & =  \{a:(1 \vee 2 \vee 3), b:(1\vee  4 \vee 5), c:(1 \vee 6 \vee 7)\}\\
\end{align*}



\noindent \textbf{Desiderata.}  In \s{X1} witnesses’ testimonies have bigger overlap, between each pair of the witnesses 2 suspects are the same, and in \s{X2} only 1 suspect is always the same. Following @Schupbach2008Alleged, one may have an intuition that the first situation is more coherent.
\vspace{2mm}\begin{description}
    \item[(\s{X1\textgreater X2})] $X_1$  should be more coherent than $X_2$.
\end{description}\vspace{2mm}



\subsection{Dice}

\textbf{The scenario.} This  scenario was offered by @Schippers2019General. You're either tossing a regular die, or a dodecahedron, $X$ is the result (there is nothing particular about this choice of dice; \emph{mutatis mutandis} this should hold for other possible pairs of dice as well). Consider the coherence of:
\[D = \{X=2, (X=2\vee X=4)\}.\]

\noindent \textbf{Desiderata.} In this scenario posterior conditional probabilities are fixed: getting 2 or 4 logically follows from getting 2 ($P(X=2\vee X=4|X=2)=1$), and you always have 50\% chance to get 2 given that the outcome was 2 or 4 ($P(X=2|X=2\vee X=4)=0.5$). Therefore, according to @Schippers2019General, the coherence of the set \s{D} shouldn't change no matter which die you use. 

\vspace{2mm}\begin{description}
    \item[(\s{D=const})] the coherence of \s{D} should not change.
\end{description}\vspace{2mm}








\section{Observations}

\subsection{Coherence scores and outcomes}

To be able to clearly see how well the existing measures of coherence deal with the mentioned desiderata, we decided to put all the results together. Our analysis extends the work of @koscholke2016evaluating.
In total we have analyzed 17 different desiderata and  7 candidates for a coherence measure (8 including ours, to be discussed later on). This required quite a lot of calculations,  so we  used  programming language \s{R} and Marco Scutari's \s{bnlearn} package to build on. Our code can be found at: TODO-repo-link.  


We represented all counterexamples as Bayesian networks. If all probabilities were defined by the author(s) of the counterexample, we used those values, otherwise we had to come up with some common-sense values. Each particular scenario is represented by a set of nodes --- usually binary ones, because they correspond to propositions which can be either true or false --- and their  appropriate instantiations. Finally, we wrote general functions for each of the  measures to  calculate the coherence scores for all the scenarios we were interested in.

In the following tables you can find coherence scores for various scenarios and measures, a  summary of  how the measures handle the desiderata, and  their success rate for this list of  challenges (\textsf{OG} stands for Olsson-Glass, \textsf{OGen} for Olsson-Glass generalized, \textsf{Sh} for Shogenji, \textsf{ShGen} for Shogenji generalized, \textsf{Fit} for Fitelson, \textsf{DM} for Douven-Meijs, \textsf{R} for Roche).


```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
load("coherencesAll.Rda")

colnames(coherencesAll) <- c("OG","OGen","Sh","ShGen","Fit","DM","R","S")

coherencesAll[,-8]  %>%  kable(format = "latex",booktabs=T, 
                         linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped","scale_down")) 

# resultsAll <- rbind(penguinsResults,
# DunnitResultsSeparate,
# JapaneseSwordsSeparateResults,
# robbersResults3,
# BeatlesResults3,
# booksResults3,
# WNarrResults,
# depthPerspectiveResults,
# dodecahedronResults)

load("resultsAll.Rda")

colnames(resultsAll) <- c("OG","OGen","Sh","ShGen","Fit","DM","R","S")

resultsAll[,-8] %>%  kable(format = "latex",booktabs=T,
                        linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped","scale_down"))

round(t(colMeans(resultsAll[,-8], na.rm = TRUE, dims = 1)),3) %>%  kable(format = "latex",booktabs=T,
                        linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped"))
```







Unfortunately, no measure was able to deal with all challenges. Note that the more recent measures, which were developed to improve on the previous ones, didn't really achieve a much higher success rate. Analysing these tables and various counterexamples, we noticed a few general issues.

\subsection{Mean}\label{sec:mean}

While the challenges have been discussed separately, considering some of them jointly also leads to an insight. Only one counterexample,  \textsf{The Beatles}, is logically inconsistent.  However, for each measure (except for the basic versions of Olsson’s and Shogenji's, which are the earliest measures and unfortunately face other serious difficulties), there is a scenario that scored lower even though it was logically consistent. For example, the generalized Olsson's measure gives a lower value to the scenario that \textsf{Tweety is a bird and a penguin} than to \textsf{The Beatles scenario}. Other measures give lower values to the scenario with a \textsf{murderer who committed both pickpocketing and robbery} than to \textsf{The Beatles}. This result disagrees with our fundamental intuition that  a coherence measure should keep track of logical consistency. 

Our hypothesis is that the  cause of this issue is as follows. When you consider the measures that face this problem, you can notice that all of them use subsets of a set (or pairs thereof) and then take the average result calculated for these subsets or pairs of subsets. However, simply taking the mean of results so obtained might be misleading, because a few low values (for the inconsistent subsets), which indicate inconsistency, might be mixed with many positive values (especially if a set is large), and taking the mean of all such results might give  a relatively high score, despite involving an inconsistency. 
Therefore, we believe that a candidate for a coherence measure shouldn't simply take mean mutual confirmation scores.



\subsection{Structure}


In the existing discussion, each scenario was represented as a set of propositions. However, it seems that usually we do not face sets of propositions but rather scenarios with some more or less explicit narration, which also indicates how the propositions are supposed to be connected. In other words, agents  not only report their object-level beliefs, but  also have some idea about their structure: which are supposed to support  which. This relation rarely is universal in the powerset of the scenario (minus the empty set of course), and so considering support between all possible pairs of propositions in the scenario in calculating coherence might be unfair towards the agent. We penalize her for lack of support even between those propositions which she never thought supported each other.   

To notice that the selection and direction of support arrows matter, consider two agents whose claims are as follows:

\vspace{2mm}
\begin{center}
\begin{tabular}{lp{9cm}}
\textsf{Agent 1}     & Tweety is a bird, more specifically a penguin. Because it’s a penguin, it doesn’t fly.  \\
\textsf{Agent 2}     &  Tweety is a bird, and because it’s a bird, it doesn't fly. Therefore Tweety is a penguin. \\
\end{tabular}
\end{center}
\vspace{2mm} 
 
\noindent Even though both of them involve the same atomic  propositions,  the first narration makes much more sense, and it seems definitely more coherent. The  approaches to coherence developed so far do not  account for this difference. 

Moreover, it  seems that when we present challenges and our intuitions about the desiderata, we implicitly assume the narration involved is the one that best   fits with our background knowledge (so, Agent 1 rather Agent 2 in the case of penguins).   However, coherence measures developed so far do not make such a fine-grained distinction between  narrations, and so the scenario which states that \textit{Tweety is BGP} (bird, grounded, penguin) gets a lower score because, quite obviously, being a bird disconfirms being a grounded animal. In such a calculation it  doesn't matter that no one even suggested this causal relationship. To illustrate this intuition, think about a picture puzzle. Just because a piece from the top right corner doesn't match a piece from the bottom left corner, it doesn't necessarily decrease the coherence of a complete picture. It just means you shouldn't evaluate how well the puzzle is prepared by putting these two pieces next to each other. 




We believe that only those  directions of support which are  indicated by the reporting  agent, or by  background knowledge, should be taken into account when measuring coherence. 





\section{Structured coherence}

Based on these observations we developed our own measure, which we call \textit{structured coherence}. In this section we will describe how we manage to avoid  the above mentioned problems.

In our calculations we use  the \s{Z} confirmation measure [see @crupi2007BayesianMeasuresEvidential, for a detailed study and defense].  It results from a normalization  of many other measures (in the sense that whichever confirmation measure you start with, after appropriate normalization you end up with \s{Z}) and has nice mathematical properties, such as ranging over $[-1,1]$ and preservation of logical entailment  and exclusion.     It is defined for hypothesis $H$ and evidence $E$ as follows:

   \begin{align*}
   \mathsf{prior} & = \pr(H) \\
   \mathsf{posterior} & = \pr(H \vert E)\\
   \mathsf{d} & = \mathsf{posterior} - \mathsf{prior} \\
       Z(\mathsf{posterior,prior}) & =  \begin{cases}
       0 & \text{if } \mathsf{prior} = \mathsf{posterior}\\
       \mathsf{d}/(1-\mathsf{prior}) & \text{if } \mathsf{posterior} > \mathsf{prior} \\
         \mathsf{d}/\mathsf{prior} & \text{o/w} 
       \end{cases}
   \end{align*}  

\noindent Of course, it might be interesting to see what would happen with the coherence calculations if other confirmation measures are plugged in, but this is beyond the scope of this paper. 


We use BNs to model the directions of support indicated in the story. A scenario is represented as a selection of instantiated nodes. We'll follow an example as we proceed.  The running example employs  the BN we constructed for the first scenario in the \textsf{Witness} problem (Figure \ref{fig:w1w2} on page \pageref{fig:w1w2}). Both witnesses testify that Steve is the murderer. Two child nodes, \textsf{W1} and \textsf{W2} correspond to the two testimonies, and as part of the narration, are to be instantiated to $1$. The root node, \textsf{D} prior to any update represents the agent's initial uncertainty about who committed the Deed (the prior distribution is uniform) and is not instantiated.





\begin{itemize}
    \item To calculate the coherence of a scenario, first  find all nodes that have at least one parent.  In the example, these are \s{W1} and \s{W2}.
    
\item For each parented node,  list all combinations  of its states and the states of its parents not excluded by the narration. We do it for \textsf{W1} in the table below this list (Table \ref{t:w1}), in the first two columns. We only consider cases in which \textsf{W1} holds, so we have 1s everywhere in the first column. However, the agent is not supposed to know who committed the deed, so all possible instantiations of \textsf{D} are listed. 

\item For each parented node and for each combination of possible states: get the prior probability of the child node and get the posterior probability of this child given the parent nodes with their fixed states. In our example the prior probability of \s{W1} is in column \textsf{priorC} (it is constant here), and the posterior probability of \textsf{W1} given different states of \textsf{D} is in column \s{post}. 



\item Use these values to calculate the \s{Z} confirmation measures. In our example, these values are in column \s{Z}. 
    
\item Get the joint probability of these parent node states. In typical cases, this is simply the prior probability. In cases in which narration nodes are actually pieces of evidence that one learns, these should be posterior probabilities obtained by instantiating the narration nodes in the BN and propagating. Such  unusual cases will be discussed when we address the \textsf{Witness} problem.   In our example, \textsf{priorA} gives the prior probabilities of various states of \textsf{D}, whereas \textsf{priorN} gives the distribution of \textsf{D} that we would obtain if we updated the BN with \textsf{W1} = \textsf{W2} = 1, that is, with the narration in question. 


\item Normalize these joint probabilities of the parent(s) so that all joint parent probabilities in the variant list add up to 1.  In the example, \textsf{weightA} is the result of normalizing \textsf{priorA} and \textsf{weightN} is the result of normalizing \textsf{priorN} (in this case, the probabilities already add up to 1, so these moves don't change anything).


\item Weight the Z score by this normalized probability, and sum these weighted Z scores, obtaining what we call the \emph{Expected Connection Strength} of the parented node under consideration.   In our example, the last two columns weight \textsf{Z} using \textsf{weightA} and \textsf{weightN} respectively.  
   In normal circumstances, \textsf{ECS} of \textsf{W1} would now be the sum of \textsf{aZ}, but --- as we discuss further on --- in this particular case we should use the updated weights, and so the \textsf{ECS} for \textsf{W1} is the sum of \textsf{nZ}.





\end{itemize}

As you can see, our approach is a bit similar to \textit{average mutual support} measures, but instead of calculating confirmation of each pair of disjoint subsets, we calculate it only for parents-child pairs. 
As a result we get a list of \textit{expected connections strengths} of each child in the BN.


\begin{table}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
load("W1calculations.Rda")
#W1calculations
#load("W1calculations.Rda")
for (c in 3:11) {
  W1calculations[,c] <- round(W1calculations[,c],3)
}
colnames(W1calculations) <- c("W1","D","priorC","post","priorA", "priorN", "weightA","weightN","Z","aZ","nZ")
W1calculations %>%  kable(format = "latex",booktabs=T,
                        linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped","scale_down"))
```
\caption{ECS calculation table for \s{W1} in the first scenario in the \s{Witness} problem.}
\label{t:w1}
\end{table}







What do we do with the list of \textsf{ECS} scores thus obtained? 
For the reasons already discussed, we don’t want to simply take the mean. The problem is a particular case of  a common problem in statistics: how to represent a set of different values in a simple way without distorting the information too much? One easy and accurate solution is to plot all values. The problem is, it gives us no unambiguous way to compare different sets.  For such tasks,  a single score is desirable.

In the context of measuring coherence of a scenario it seems that we should pay special attention to  \s{mean}, \s{sd}, and \s{minimum}.  The mean gives us an idea of how strong  the average support between the elements is. Standard deviation informs us how stable  the support between elements is (the higher  \s{sd}, the more cautious we need to be about the mean). The minimum represents the weakest point in the narration. Note that it can be far from the mean --- this is important, because even a single very weak point destroys the coherence of a whole story.\footnote{ This is a conservative score in the following sense. Imagine two narrations. In the first one, you have a case where all parent-child relations  except one get the maximal positive score. The remaining one  gets the score of  -1. Then the overall score will be -1.
In  the second narration all the relations take a value close to -1. It  will have a higher overall score  than -1.  On this approach, the second narration is more coherent than the first one. We find this property desirable: the presence of an element with the posterior that equals 0 (which is needed for $Z$ confirmation being -1) kills the coherence of an otherwise strong narration, and a narration containing such an element is in worse standing than simply a very unlikely one.}

We developed a function which uses these 3 values:

\footnotesize
\begin{align*}
\mathsf{Structured}(\mathsf{ECS}) & = \begin{cases}
\left[\mathsf{mean}(\mathsf{ECS}) - \textsf{sd}(\mathsf{ECS})\right] \times \left(\textsf{min}(\mathsf{ECS})+1 \right) - \textsf{min}(\mathsf{ECS})^2   & \text{if } min(\mathsf{ECS})<0 \\\mathsf{mean}(\mathsf{ECS}) - \textsf{sd}(\mathsf{ECS}) & \text{o/w}
\end{cases}
\end{align*}
\normalsize


The intuitions behind it are as follows. If all values are non-negative, i.e. each relation between parents and a child is supportive, then even the weakest point of a story is high enough not to care about it. In such cases we take $\s{mean} - \s{sd}$ as the final result.


One way to look at the first case is to think of it as a weighted average of $\s{mean} - \s{sd}$ and $\s{min}(\s{ECS})$, $\s{min}$ for brevity.  The weight assigned to $\s{min}$ is $\vert \s{min} \vert$. This is fairly natural: if the minimum is -1, we want to give it full weight, $1 = \vert \s{min}\vert = - \s{min}$. The weight assigned to $\s{mean} - \s{sd}$ is the rest, $1-\vert \s{min}\vert = 1 - (- \s{min}) = 1+\s{min}$. 


This function has a desired property which was missing in most of the other coherence measures. Whenever we encounter a logically inconsistent story, i.e. a story with the lowest possible minimum (in our measure it is -1), we'll end up with -1 also as the final score. The achieved results are also plausible if the minimum is close to the lowest possible value.


Another desired feature is that if there are some negative \s{ECS} values,  we need to be cautious about the weakest point.  The lower the minimum, the less attention we should pay to $\s{mean} - \s{sd}$. So, for instance, if the minimum is -0.8, the weight of  $\s{mean} - \s{sd}$ should be $-0.8+1 = 0.2$, while if it is $-0.2$, the weight is $0.8$. Now, let's take this coherence measure for a ride. 
























\section{Handling counterexamples}



Using the notation and desiderata already introduced, let's go over BNs for the counterexamples involved, and  use the counterexamples to evaluate the performace of the new measure.


\subsection{Penguins}


We used the distribution used in the original formulation to build three BNs corresponding to the narrations at play (Fig. \ref{fig:BGP}-\ref{fig:BP}).\footnote{Not without concerns. There are around 18 000 species of birds, and around 60 of them are flightless. We  couldn't find information about counts, but it seems the probability of being a penguin if one is grounded is overestimated by philosophers.  Also, there are many things that are not grounded but are not birds, mostly insects, and there's plenty of them. We did spend some time coming up with plausible ranges of probabilities to correct for such factors, and none of them actually makes a difference to the main point. So, for the sake of simplicity, we leave the original unrealistic distribution in our discussion.}






\begin{figure}
\hspace{2cm}\scalebox{0.8}{\begin{subfigure}[!ht]{0.4\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE,  fig.show = "hold", out.width = "100%", dpi = 300}
source("bns//Penguins.R")
graphviz.plot(BirdDAGbgp)
```
\end{subfigure}} \hfill
\hspace{-3cm}\begin{subfigure}[!ht]{0.7\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "70%", dpi = 300}
source("utils//kableCPTs.R")
CPkable0("BirdBNbgp","B")
CPkable1("BirdBNbgp","P")
CPkable2("BirdBNbgp","G")
```
\end{subfigure}
\caption{Bayesian network for the \textsf{BGP} scenario.}
\label{fig:BGP}
\end{figure}




\begin{figure}
\hspace{2cm}\scalebox{0.6}{
\begin{subfigure}[!ht]{0.4\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "70%",  dpi = 300, fig.height=2.5, fig.width=2}
graphviz.plot(BirdDAGbg)
```
\end{subfigure} }
\hfill
\hspace{-3cm}\begin{subfigure}[!ht]{0.7\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "70%", dpi = 300}
CPkable0("BirdBNbg","B")
CPkable1("BirdBNbg","G")
```
\end{subfigure}
\label{fig-BG}
\caption{Bayesian network for the \textsf{BG} scenario.}
\end{figure}





\begin{figure}
\scalebox{0.6}{
\hspace{4cm}\begin{subfigure}[!ht]{0.4\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", dpi = 300, fig.width=2, fig.height= 2.5}
graphviz.plot(BirdDAGbp)
```
\end{subfigure}} \hfill
\hspace{-3cm}\begin{subfigure}[!ht]{0.7\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "70%", dpi = 300}
CPkable0("BirdBNbp","B")
CPkable1("BirdBNbp","P")
```
\end{subfigure}
\caption{Bayesian network for the \textsf{BP} scenario.}
\label{fig:BP}
\end{figure}
\newpage 

Now, let's calculate the coherence scores and see if the desiderata are satisfied (the abbreviations we already used are as before, and \textsf{S} stands for \emph{Structured}, our coherence measure):


```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
load("calculations/RdataObjects/penguinsTablePaper.Rda")
load("calculations/RdataObjects/penguinsResultsPaper.Rda")

colnames(penguinsTable) <- c("OG","OGen","Sh","ShGen","Fit","DM","R","S")
colnames(penguinsResults) <- c("OG","OGen","Sh","ShGen","Fit","DM","R","S")

round(penguinsTable,3) %>%  kable(format = "latex",booktabs=T,
                        linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped","scale_down"))

penguinsResults %>%  kable(format = "latex",booktabs=T,
                        linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped","scale_down"))
```









## Dunnit

Here, we deal with two separate BNs. One, before the \textsf{Twin} node is even considered (Fig. \ref{fig:twinless}), and one with the \textsf{Twin} node (Fig. \ref{fig:twin}).  






The CPTs for the no-twin version are in agreement with those in the ones in the Twin case. Since the original example didn't specify exact probabilities, we came up with some plausible values.





\begin{figure}

\scalebox{1.7}{
\begin{subfigure}[!ht]{0.3\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE,  fig.show = "hold", out.width = "70%", dpi = 300}
graphviz.plot(DunnitNoTwinDAG, layout = "twopi")
```
\end{subfigure}} 
\hspace{-0.8cm}\begin{subfigure}[!ht]{0.2\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE,  fig.show = "hold", out.width = "70%", dpi = 300}
CPkable0("DunnitNoTwinBN","M")
CPkable1("DunnitNoTwinBN","G")
```
\end{subfigure}  
 \hspace{0.5cm}\begin{subfigure}[!ht]{0.2\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "70%", dpi = 300}

CPkable1("DunnitNoTwinBN","I")
CPkable1("DunnitNoTwinBN","W")
```
\end{subfigure}
\caption{Twin-less BN for the \textsf{Dunnit} problem.}
\label{fig:twinless}
\end{figure}







\begin{figure}
\scalebox{1.7}{
\begin{subfigure}[!ht]{0.4\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE,  fig.show = "hold", out.width = "70%", dpi = 300}
graphviz.plot(DunnitDAG, layout = "twopi")
```
\end{subfigure}} 
\hspace{-1cm}\begin{subfigure}[!ht]{0.3\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "70%", dpi = 300}
CPkable2("DunnitBN","W")
```
\end{subfigure}
\caption{BN for the \textsf{Dunnit} problem. The key difference for the twin version lies in the construction of the CPT for \textsf{W}. The table gives conditional probabilities for \textsf{W} given various joint states of \textsf{Tw} and \textsf{G}.}
\label{fig:twin}
\end{figure}

\newpage

Coherence calculations result in the following:

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE,  fig.show = "hold", out.width = "70%", dpi = 300}
load("calculations/RdataObjects/DunnitTableSeparate.Rda")
load("calculations/RdataObjects/DunnitResultsSeparate.Rda")


colnames(DunnitTableSeparate) <- c("OG","OGen","Sh","ShGen","Fit","DM","R","S")
colnames(DunnitResultsSeparate) <- c("OG","OGen","Sh","ShGen","Fit","DM","R","S")


round(DunnitTableSeparate,3) %>%  kable(format = "latex",booktabs=T,
                        linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped","scale_down"))

DunnitResultsSeparate %>%  kable(format = "latex",booktabs=T,
                        linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped","scale_down"))
```








## Japanese swords


 There is a common DAG for the three scenarios, but the CPTs differ (Fig. \ref{fig:japanese}).









\begin{figure}
\scalebox{0.5}{
\hspace{2.5cm}\begin{subfigure}[!ht]{0.4\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE,  fig.show = "hold", out.width = "70%", dpi = 300, fig.width=2}
source("bns//Witness.R")
graphviz.plot(JapDAG)
```
\end{subfigure}} \hfill
\begin{subfigure}[!ht]{0.6\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE,  fig.show = "hold", out.width = "70%", dpi = 300}
CPkable0("Jap1BN","J")
CPkable1("Jap1BN","O")
```
\caption{Scenario 1.}
\end{subfigure} 
\begin{subfigure}[!ht]{0.4\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE,  fig.show = "hold", out.width = "70%", dpi = 300}
CPkable0("Jap2BN","J")
CPkable1("Jap2BN","O")
```
\caption{Scenario 2.}
\end{subfigure}  \hfill
\begin{subfigure}[!ht]{0.6\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE,  fig.show = "hold", out.width = "70%", dpi = 300}
CPkable0("Jap3BN","J")
CPkable1("Jap3BN","O")
```
\caption{Scenario 3.}
\end{subfigure} 
\caption{A common DAG and three sets of CPTs for the \textsf{Japanese Swords} problem.}
\label{fig:japanese}
\end{figure}

\newpage 

Coherence calculations yield:

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE,  fig.show = "hold", out.width = "70%", dpi = 300}
load("calculations/RdataObjects/JapaneseSwordsSeparateTable.Rda")
load("calculations/RdataObjects/JapaneseSwordsSeparateResults.Rda")


colnames(JapaneseSwordsSeparateTable) <- c("OG","OGen","Sh","ShGen","Fit","DM","R","S")
colnames(JapaneseSwordsSeparateResults) <- c("OG","OGen","Sh","ShGen","Fit","DM","R","S")


round(JapaneseSwordsSeparateTable,3) %>%  kable(format = "latex",booktabs=T,
                        linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped","scale_down"))

JapaneseSwordsSeparateResults %>%  kable(format = "latex",booktabs=T,
                        linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped","scale_down"))
```





##  Robbers

The robbers counterexample involves a phenomenon we've already seen: it is not clear whether  the information about the prior probabilities is supposed to be part of the narration or not. If we want to include this information in our coherence assessment, we can do this employing a single BN.





\begin{figure}
\begin{subfigure}[!ht]{0.4\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", dpi = 300}
source("bns//Robbers.R")
graphviz.plot(robbersDAGsimplified)
```
\end{subfigure} \hfill
\begin{subfigure}[!ht]{0.6\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", fig.width = 1, dpi = 300}
 round(whoMurderedProb,3)  %>%   kable(format = "latex",booktabs=T,
      linesep = "", escape = FALSE, col.names = c("Pr")) %>% kable_styling(latex_options=c("striped"),font_size = 9, full_width =  FALSE)  


table <- MIsPProb %>%   kable(format = "latex",booktabs=T,
      linesep = "", escape = FALSE) %>% kable_styling(latex_options=c("striped"),font_size = 9, full_width =  FALSE) 
  
add_header_above(table, c("MisP","WhoMurdered"=3), line = FALSE)

  
table <- MIsRProb %>%   kable(format = "latex",booktabs=T,
      linesep = "", escape = FALSE) %>% kable_styling(latex_options=c("striped"),font_size = 9, full_width =  FALSE) 
  
add_header_above(table, c("MisR","WhoMurdered"=3), line = FALSE)

```
\end{subfigure}
\caption{BN for the \textsf{Robbers} problem.}
\label{fig:Robbers}
\end{figure}

\newpage


Coherence calculations yield the following results:




```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", fig.height= 2, fig.width= 2, out.width = "90%"}
load("calculations/RdataObjects/robbersTable3.Rda")
load("calculations/RdataObjects/robbersResults3.Rda")

colnames(robbersTable3) <- c("OG","OGen","Sh","ShGen","Fit","DM","R","S")
colnames(robbersResults3) <- c("OG","OGen","Sh","ShGen","Fit","DM","R","S")



round(robbersTable3,3) %>% kable(format = "latex",booktabs=T,
                        #col.names = c(node, "1", "0"),
                        linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("scale_down"))

robbersResults3 %>% kable(format = "latex",booktabs=T,
                        #col.names = c(node, "1", "0"),
                        linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("scale_down"))
```



































##  The Beatles



\begin{figure}
\scalebox{1.6}{
\begin{subfigure}[!ht]{0.4\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE,  fig.show = "hold", out.width = "70%", dpi = 300}
source("bns//Beatles.R")
graphviz.plot(BeatlesDAG)

```
\end{subfigure}} \hfill
\begin{subfigure}[!ht]{0.4\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "70%", dpi = 300}
CPkable0("BeatlesBN","G")
#CPkable1("BirdBNbgp","P")
#CPkable2("BirdBNbgp","G")
```
\end{subfigure}
\caption{Bayesian network for the \textsf{Beatles} scenario.}
\end{figure}

We assume the prior probability of each individual band member being dead to 0.5 (as in the above table), and the CPT for \textsf{D} is many-dimensional and so difficult to present concisely, but the method is straightforward: probability 1 is given to \textsf{D} in all combinations of the parents in which exactly one is true, and otherwise \textsf{D} gets conditional probability 0.


Coherence calculations give the following results:









```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
load("calculations/RdataObjects/BeatlesTable3.Rda")
load("calculations/RdataObjects/BeatlesResults3.Rda")


colnames(BeatlesTable3) <- c("OG","OGen","Sh","ShGen","Fit","DM","R","S")
colnames(BeatlesResults3) <- c("OG","OGen","Sh","ShGen","Fit","DM","R","S")


round(BeatlesTable3,3) %>%  kable(format = "latex",booktabs=T,
                        linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped","scale_down"))

BeatlesResults3 %>%  kable(format = "latex",booktabs=T,
                        linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped","scale_down"))
```





## Alicja and books



The BN is fairly straightforward (Fig. \ref{fig:books}) and the results are as follows:



\begin{figure}

\hspace{20mm}
\scalebox{0.5}{\begin{subfigure}[!ht]{0.4\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE,  fig.show = "hold", dpi = 300,fig.align="center"}
source("bns//Books.R")
graphviz.plot(BooksDAG)

```
\end{subfigure}} \hfill
\begin{subfigure}[!ht]{0.6\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "70%", dpi = 300}
CPkable0("BooksBN","A")
CPkable1("BooksBN","R")
#CPkable2("BirdBNbgp","G")
```
\end{subfigure}
\caption{Bayesian network for the \textsf{Books} problem.}
\label{fig:books}
\end{figure}










```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
load("calculations/RdataObjects/booksTable3.Rda")
load("calculations/RdataObjects/booksResults3.Rda")


colnames(booksTable3) <- c("OG","OGen","Sh","ShGen","Fit","DM","R","S")
colnames(booksResults3) <- c("OG","OGen","Sh","ShGen","Fit","DM","R","S")

round(booksTable3,3) %>%  kable(format = "latex",booktabs=T,
                        linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped","scale_down"))

booksResults3 %>%  kable(format = "latex",booktabs=T,
                        linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped","scale_down"))
```
































## The witnesses

Two requirements are associated with this example: both $\{$\textsf{W1, W2}$\}$ and $\{$\textsf{W4, W5}$\}$ should be more coherent than $\{$\textsf{W3, W4}$\}$.  The basic idea behind the CPTs we used  is that for any particular witness we take the probability of them including the perpetrator in their list to be 0.8, and the probability of including an innocent to be .05. Of course, the example can be run with different conditional probability tables. Let's first take a look at the BN for the first scenario (Fig. \ref{fig:w1w2}). 





\begin{figure}
\scalebox{1.2}{
\hspace{1cm}\begin{subfigure}[!ht]{0.4\textwidth}
```{r,echo=FALSE,eval=TRUE, fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%",  dpi = 300}
graphviz.plot(W12DAG)
```
\end{subfigure}} \hfill
\begin{subfigure}[!ht]{0.3\textwidth}
```{r,echo=FALSE,eval=TRUE, fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%",  dpi = 300}
Dcpt <- as.data.frame(W12BN$D[[4]])
Dcpt$Freq <- round(Dcpt$Freq,3)
colnames(Dcpt) <-  c("D", "Pr")
Dcpt %>%  kable(format = "latex",booktabs=T, linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped"))
```
\end{subfigure}
\centering
\begin{subfigure}[!ht]{0.3\textwidth}
```{r,echo=FALSE,eval=TRUE, fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%",  dpi = 300}
CPkable1("W12BN","W1") %>%   kable_styling(latex_options=c("striped"))
```
\end{subfigure}
\caption{BN for the \textsf{W1W2} narration in the \textsf{Witness} problem. CPT for \textsf{W2} is identical to the one for \textsf{W1}.}
\label{fig:w1w2}
\end{figure}


The CPT for \textsf{D} is  uniform.  The table for \textsf{W1} provides the conditional probability of \textsf{W1} listing (\textsf{W1}=1) or not listing  (\textsf{W1}=0) a particular person given that the actual value of \textsf{D} is Steve/Martin/\dots. The underlying rule is: if someone is guilty, a witness will mention them with probability $.8$, and if they aren't, they will be listed with probability $.05$.  In the remaining two BNs for the problem the CPT for \textsf{D} remains the same, and the CPTs for the witness nodes are analogous to the one for \textsf{W1}. The remaining BNs have the following obvious DAGs (Fig. \ref{fig:witness}).


\begin{figure}\centering
\scalebox{1.2}{
\begin{subfigure}[!ht]{0.4\textwidth}
```{r,echo=FALSE,eval=TRUE, fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%",  dpi = 300}
graphviz.plot(W34DAG)
```
\end{subfigure}}

\scalebox{1.2}{\begin{subfigure}[!ht]{0.4\textwidth}
```{r,echo=FALSE,eval=TRUE, fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%",  dpi = 300}
graphviz.plot(W45DAG)
```
\end{subfigure}}
\caption{Two remaining DAGs for the \textsf{Witness} problem.}
\label{fig:witness}
\end{figure}






\newpage


<!-- If we calculate the coherence and evaluate the desiderata, however, we obtain the following results. -->


<!-- ```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"} -->
<!-- load("calculations/RdataObjects/witnessTable2.Rda") -->
<!-- load("calculations/RdataObjects/witnessResults2.Rda") -->

<!-- witnessTable2 %>%  kable(format = "latex",booktabs=T, -->
<!--                         linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped","scale_down")) -->

<!-- witnessResults2 %>%  kable(format = "latex",booktabs=T, -->
<!--                         linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped","scale_down")) -->
<!-- ``` -->

We think that what this example illustrates is that  we should really carefully think about whose cognitive perspective is taken when we represent a narration using a BN, focusing on whether the BN involves nodes which are not part of the narration whose coherence is to be evaluated. In particular, the probabilistic information about the uniform distribution of  guilt probability is not part of any of the three involved narrations, but rather a part of a third-person set-up prior to obtaining any evidence. 

To evaluate the coherence of a narration, at least for unmentioned assumptions that one doesn't have strong independent reasons to keep, one should think counterfactually, granting the consequences of the narration and asking what would happen if it indeed was true. In our case, a judge who evaluates the coherence of witness testimonies once she has heard them, no longer thinks that the distribution of \textsf{D} is uniform. And this agrees with the counterfactual strategy we just described: it is a consequence of the probabilistic set-up and the content of \textsf{W1} and \textsf{W2} that if \textsf{W1} and \textsf{W2} were true, the distribution for \textsf{D} no longer would be uniform, and so it is unfair to judge the coherence of this scenario without giving up this assumption and updating one's assumptions about \textsf{D}. 

In such a case, we think, we should   update  \textsf{D}   to what it would be had \textsf{W1} and \textsf{W2} be instantiated with 1s:


```{r,echo=FALSE,eval=TRUE, fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%",  dpi = 300}
updatedW12BN <- updateBN(W12BN,c("W1","W2"), c("1","1"))

D12cpt <- as.data.frame(updatedW12BN$D[[4]])
D12cpt$`updatedW12BN$D[[4]]` <- round(D12cpt$`updatedW12BN$D[[4]]`,3)
colnames(D12cpt) <-  c("Pr")

t(D12cpt) %>%  kable(format = "latex",booktabs=T, linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped"))
```

\noindent and use these updated probabilities to build the weights used in our coherence calculations for this narration (and proceed accordingly, instead updating on another set of  narration nodes in the  coherence evaluation of other narrations).\footnote{Note  however that  you should not simply instantiate the BN with \textsf{W1} and \textsf{W2}, propagate and run the coherence calculations on the updated BN. Then both these nodes would get 1s in their respective CPTs and coherence calculations would make   all  confirmation measures involved in such calculations  based on posterior probability equal 1. If narration members have probability one, no other information will be able to confirm it.}  Once this strategy is taken,  the problem turns out to be not that challenging for any of the coherence measures under discussion.






```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
load("calculations/RdataObjects/WNarrTable.Rda")
load("calculations/RdataObjects/WNarrResults.Rda")


colnames(WNarrTable) <- c("OG","OGen","Sh","ShGen","Fit","DM","R","S1", "S")
colnames(WNarrResults) <- c("OG","OGen","Sh","ShGen","Fit","DM","R","S1","S")

WNarrTable <- WNarrTable[,-8]

round(WNarrTable,3) %>%  kable(format = "latex",booktabs=T,
                        linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped","scale_down"))

WNarrResults <- WNarrResults[,-8]

WNarrResults %>%  kable(format = "latex",booktabs=T,
                        linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped","scale_down"))
```





## Depth

 We start with representing the two scenarios with two fairly natural BNs (\textsf{C} stands for who Committed the crime, \textsf{TXYZ} stands for Testimony that $X\vee Y \vee Z$), see Fig. \ref{fig:dod1} and \ref{fig:dod2}.





\begin{figure}
\scalebox{1.7}{
\begin{subfigure}[!ht]{0.4\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE,  fig.show = "hold", out.width = "70%", dpi = 300}
source("bns//Depth.R")
graphviz.plot(DX1DAG)
```
\end{subfigure}} 
\hspace{-1cm}\scalebox{0.6}{\begin{subfigure}[!ht]{0.3\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "70%", dpi = 300}
CPkable0("DX1BN","C")
CPkable1("DX1BN","T123")
CPkable1("DX1BN","T124")
CPkable1("DX1BN","T134")
```
\end{subfigure}}
\caption{BN for \textsf{X1} in the \textsf{Depth} problem.}
\label{fig:dod1}
\end{figure}




\begin{figure}
\scalebox{1.7}{
\begin{subfigure}[!ht]{0.4\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE,  fig.show = "hold", out.width = "70%", dpi = 300}
graphviz.plot(DX2DAG)
```
\end{subfigure}} 
\hspace{-1cm}\scalebox{0.6}{\begin{subfigure}[!ht]{0.3\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "70%", dpi = 300}
CPkable0("DX2BN","C")
CPkable1("DX2BN","T123")
CPkable1("DX2BN","T145")
CPkable1("DX2BN","T167")
```
\end{subfigure}}
\caption{BN for \textsf{X2} in the \textsf{Depth} problem.}
\label{fig:dod2}
\end{figure}



\newpage


One effect of dropping the "the witness testified that" and using the testimony contents themselves is that the CPTs for the narration nodes are deterministically connected with the root node. In result, the coherence calculations give in the following:

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE,  fig.show = "hold", out.width = "70%", dpi = 300}
load("calculations/RdataObjects/depthTableNarr.Rda")
load("calculations/RdataObjects/depthResultsNarr.Rda")
load("calculations/RdataObjects/WNarrTable.Rda")




colnames(depthTableNarr) <- c("OG","OGen","Sh","ShGen","Fit","DM","R","S1", "S2")
colnames(depthResultsNarr) <- c("OG","OGen","Sh","ShGen","Fit","DM","R","S1","S2")


#colnames(depthTableNarr)[8:9] <- c("Structured 1", "Structured 2")
#colnames(depthResultsNarr)[8:9] <- c("Structured 1", "Structured 2")

round(depthTableNarr,3) %>%  kable(format = "latex",booktabs=T,
                        linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped","scale_down"))

depthResultsNarr %>%  kable(format = "latex",booktabs=T,
                        linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped","scale_down"))
```

Note that this time we listed two values for our measure. \textsf{Structured 1} shows the values obtained if we do not update the weighting of the node not included in the narration, and \textsf{Structured 2} is the result of such an updated weighing (analogous to the updating involved in the \textsf{Witness} problem). Now, what are we to make of this?

\textsf{Structured 1} is negative. This isn't too surprising: after all, this is the coherence of the narration with the probabilistic assumption that the distribution for \textsf{C} is uniform, and this probabilistic assumption undermines the narration. Why, however, does \textsf{Structured 2} equal 1, and why are the results identical for both narrations?  This, upon reflection, isn't too suprising either. If the BN and the narration is supposed to represent a single agent's credal state, there is only one state of \textsf{C} in which the whole narration $X_1$ is true -- trivially, it is the one in which suspect 1 is guilty, and it is the same unique state of \textsf{C} in which the whole narration $X_2$ is true.  Since seen as narrations these sets have exactly the same truth conditions, there is no surprise in them being equally coherent. 




What if the sentences in the set are not claims made by one agent and there is no single underlying credal state? We aren't convinced that  our tool is optimal for measuring the  agreement of multiple witnesses.  Instead, there already exists a working measure of such an agreement ---  Cohen's $\kappa$ -- which already gives the desired results.


To illustrate, let's think of a simplified situation (devoid of three-dimensional tables) with two witnesses $w1$ and $w2$, where the respective sets are 
$A  = \{1 \vee 2 \vee 3, 1\vee 2 \vee 4\}$ and 
$B  =  \{1 \vee 2 \vee 3, 1\vee  4 \vee 5\}$ and  in each set the first proposition comes from $w1$ and the second from $w2$.   The information for these two sets can be tabulated as follows:

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", fig.height= 2, fig.width= 2, out.width = "90%"}
A <- as.table(matrix(c(2,1,1,4),byrow=TRUE,ncol=2))
rownames <- c("w1:suspect","w1: innocent")
colnames <- c("w2: suspect","w2: innocent")
dimnames(A) <- list("w1" = rownames,"w2" = colnames)
Akable <- A %>% kable(format = "latex",booktabs=T,
                        linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped"))
Akable

B <- as.table(matrix(c(1,2,2,3),byrow=TRUE,ncol=2))
rownames <- c("w1: suspect","w1: innocent")
colnames <- c("w2: suspect","w2: innocent")
dimnames(B) <- list("w1" = rownames,"w2" = colnames)
Bkable <- B %>% kable(format = "latex",booktabs=T,
            #col.names = c(node, "1", "0"),
                        linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped"))
Bkable
```
\noindent Standard calculations using the \textsf{vcd} package results in the following unweighted values of Cohen's $\kappa$.



```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", fig.height= 2, fig.width= 2, out.width = "90%", message = FALSE, warning = FALSE}
library(vcd)
kappas <- cbind(as.data.frame(Kappa(A)[1]),
                as.data.frame(Kappa(B)[1]))[1,]
kappas2 <- round(kappas,3)
colnames(kappas2) <- c("A","B")
kappas2 %>% kable(format = "latex",booktabs=T,
      linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped"))
```



Let's further illustrate  our point about the  requirement that the BN should represent a single agent's cognitive state. For instance, you can represent, the situation in $A$ from the perspective of the first witness. This suggests we should focus only on the nodes involved in the narration, and on the fact that from the witness' perspective the suspects are not equally likely. The example doesn't provide us enough information to build a table for \textsf{C}. In fact, no information about the wintess attitude towards this node is given, but given they say what they say, it's unlikely they think the distribution is uniform. So let's take one of the witness' own statements as the root (which ones we choose doesn't change the outcome). Clearly (or, at least, hopefully, if we talk about witnesses), the agent thinks her own claim is very likely and evaluates the probability of the other statements in $A$ or $B$ from its perspective. This gives us two different BNs, and when we calculate the respective coherences we actually do get the desired result, which isn't too hard for the other measures either. 



\begin{figure}
\hspace{2cm}\scalebox{0.6}{
\begin{subfigure}[!ht]{0.3\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", dpi = 300}
source("bns//Depth.R")
graphviz.plot(T123DAG)
```
\end{subfigure} }\hfill
\begin{subfigure}[!ht]{0.6\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", fig.width = 1, dpi = 300}
CPkable0("T123BN","T123")
table <- round(T124Apr,3)  %>%   kable(format = "latex",booktabs=T,
      linesep = "", escape = FALSE) %>% kable_styling(latex_options=c("striped"),font_size = 9, full_width =  FALSE)  
add_header_above(table, c("T124","T123"=2), line = FALSE)
```
\end{subfigure}
\caption{A witness perspective for the \textsf{agreement} problem, set $A$.}
\end{figure}



\begin{figure}
\hspace{2cm}\scalebox{0.6}{
\begin{subfigure}[!ht]{0.3\textwidth}

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%",  dpi = 300}
graphviz.plot(T123DAG2)
```
\end{subfigure}} \hfill
\begin{subfigure}[!ht]{0.6\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", fig.width = 1, dpi = 300}
CPkable0("T123BN","T123")
table <- round(T145Bpr,3)  %>%   kable(format = "latex",booktabs=T,
      linesep = "", escape = FALSE) %>% kable_styling(latex_options=c("striped"),font_size = 9, full_width =  FALSE)  
add_header_above(table, c("T124","T123"=2), line = FALSE)
```
\end{subfigure}
\caption{A witness perspective for the \textsf{agreement} problem, set $B$.}
\end{figure}





\newpage



```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
load("calculations/RdataObjects/depthPerspectiveTable.Rda")
load("calculations/RdataObjects/depthPerspectiveResults.Rda")


colnames(depthPerspectiveTable) <- c("OG","OGen","Sh","ShGen","Fit","DM","R","S")
colnames(depthPerspectiveResults) <- c("OG","OGen","Sh","ShGen","Fit","DM","R","S")


round(depthPerspectiveTable,3) %>%  kable(format = "latex",booktabs=T,
                        linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped","scale_down"))


depthPerspectiveResults %>%  kable(format = "latex",booktabs=T,
                        linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped","scale_down"))

```



## Dice

We'll follow the strategy similar to the one we already used. Since neither the example nor the narrations involve information about how probable it is that we're dealing with a regular die, as opposed to a dodecahedron, we avoid using a node representing this. Moreover, if at a given time the agent claims that the result is both two and (two or four), their cognitive situation at that time cannot be represented using uniform distribution for possible toss outcomes. Instead, we start with initial separate BNs for a regular die and a dodecahedron which do have uniform distributions for the \textsf{O} (outcome) node (Fig. \ref{fig:diceBN}), but when weighing the antedecent nodes which are not strictly speaking part of the narration, we use the probabilities updated in light of the narration content itself. 




\begin{figure}
\scalebox{1.3}{\begin{subfigure}[!ht]{0.3\textwidth}
```{r,echo=FALSE,eval=TRUE, fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", dpi = 300}
source("bns//Dodecahedron.R")
graphviz.plot(DodDAG)
```
\end{subfigure}}  
\hspace{0.2cm}\begin{subfigure}[!ht]{0.25\textwidth}
```{r,echo=FALSE,eval=TRUE, fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", fig.height = 1.5, dpi = 300, fig.width = 1.5}
source("utils/kableCPTs.R")
CPkable0("RegularBN","O")
```
\caption{Root CPT for the regular die.}
\end{subfigure} 
\hspace{0.2cm} \begin{subfigure}[!ht]{0.25\textwidth}
```{r,echo=FALSE,eval=TRUE, fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", fig.height = 1.5, dpi = 300, fig.width = 1.5}
CPkable0("DodecahedronBN","O")
```
\caption{Root CPT for the dodecahedron.}
\end{subfigure}

\begin{subfigure}[!ht]{0.25\textwidth}
```{r,echo=FALSE,eval=TRUE, fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", dpi = 300}
CPkable1("RegularBN","T")
CPkable1("RegularBN","TF")
```
\caption{Conditional probabilities for the regular die.}
\end{subfigure} \hfill
\scalebox{0.8}{\begin{subfigure}[!ht]{0.7\textwidth}
```{r,echo=FALSE,eval=TRUE, fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", dpi = 300}
CPkable1("DodecahedronBN","T")
CPkable1("DodecahedronBN","TF") 
```
\caption{\large Conditional probabilities for the dodecahedron.}
\end{subfigure}} 
\caption{BNs for the \textsf{dice} problem.}
\label{fig:diceBN}
\end{figure}

\newpage

Calculation of coherences of the scenarios in the respective BNs yield the following result:



```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
load("calculations/RdataObjects/diceTablePaper.Rda")
load("calculations/RdataObjects/dodecahedronResultsNew.Rda")




colnames(DiceTableNew) <- c("OG","OGen","Sh","ShGen","Fit","DM","R","S1", "S")
colnames(dodecahedronResults) <- c("OG","OGen","Sh","ShGen","Fit","DM","R","S1","S")


DiceTableNew <- DiceTableNew[,-8]
#colnames(DiceTableNew)[8] <- "Structured"

dodecahedronResults <- dodecahedronResults[,-8]
#colnames(dodecahedronResults)[8] <- "Structured"


round(DiceTableNew,3) %>%  kable(format = "latex",booktabs=T,
                        linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped","scale_down"))


dodecahedronResults %>%  kable(format = "latex",booktabs=T,
                        linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped","scale_down"))


```






<!-- \hfill -->
<!-- \begin{subfigure}[!ht]{0.3\textwidth} -->
<!-- ```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", fig.height = 2, dpi = 300, fig.width = 1.5} -->
<!-- graph::nodeRenderInfo(plotW34) <- list(fontsize= 7) -->
<!-- Rgraphviz::renderGraph(plotW34) -->
<!-- ``` -->
<!-- \end{subfigure} \hfill -->
<!-- \begin{subfigure}[!ht]{0.3\textwidth} -->
<!-- ```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", fig.height = 2, dpi = 300, fig.width = 1.5} -->
<!-- graph::nodeRenderInfo(plotW45) <- list(fontsize= 7) -->
<!-- Rgraphviz::renderGraph(plotW45) -->
<!-- ``` -->
<!-- \end{subfigure} -->
<!-- \caption{Three BNs  for a  third-person perspective in the \textsf{witness} problem. CPTs for the remaining nodes preserved from the initial BN.} -->
<!-- \end{figure} -->

The measure that we think is appropriate here yields the same result, 1, for both situations. Come to think of it, we don't find this counterintuitive. These are two sentences one of which is a trivial consequence of the other. 




#  Conclusions

Ultimately, all the coherence results and desiderata yield the following two tables and success rates:

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
# coherencesAll <- rbind(round(penguinsTable,3), 
# round(DunnitTableSeparate,3), round(JapaneseSwordsSeparateTable,3),
# round(robbersTable3,3),
# round(BeatlesTable3,3),
# round(booksTable3,3),
# round(WNarrTable,3),
# round(depthPerspectiveTable,3),
# round(DiceTableNew,3)
# )

load("coherencesAll.Rda")



colnames(coherencesAll) <- c("OG","OGen","Sh","ShGen","Fit","DM","R","S")


coherencesAll  %>%  kable(format = "latex",booktabs=T, 
                         linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped","scale_down")) 

# resultsAll <- rbind(penguinsResults,
# DunnitResultsSeparate,
# JapaneseSwordsSeparateResults,
# robbersResults3,
# BeatlesResults3,
# booksResults3,
# WNarrResults,
# depthPerspectiveResults,
# dodecahedronResults)

load("resultsAll.Rda")


colnames(resultsAll) <- c("OG","OGen","Sh","ShGen","Fit","DM","R","S")

resultsAll %>%  kable(format = "latex",booktabs=T,
                        linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped","scale_down"))



#save(resultsAll, file = "resultsAll.Rda")



round(t(colMeans(resultsAll, na.rm = TRUE, dims = 1)),3) %>%  kable(format = "latex",booktabs=T,
                        linesep = "",  escape = FALSE) %>%   kable_styling(latex_options=c("striped"))


#getwd()
```



Let’s recap what we've done. We introduced the most prominent coherence measures and a number of counterexamples put forward against them.  Then, we pointed out some common problems they face. These observations helped us develop our own measure. It improves on the existing approaches by using the structure of BNs, and by doing something a bit more sophisticated than taking means. Finally, we argued that this way we managed to avoid many counterexamples that were problematic for other measures. This, in fact turned out to be a balancing act: we agreed with many intuitions behind the counterexamples had doubts about some of them, and a few of the cases needed somewhat more elaborate reflection before our measure gave the desired outcome.  We end with a list of tasks for further reearch.



 One issue that needs further study is whether the structured coherence measure yields desired results in more straightforward cases as compared with empirical results on how real agents assess coherence.  Another question is how the measure handles legal cases for which BNs have already been developed [@vlek2013modeling, @vlek2014building, @vlek2015, @vlek2016stories, @fenton2013GeneralStructureLegal, @fenton2013GeneralStructureLegal]. It might also be worthwile to investigate what happens if confirmation measures other than \s{Z} are plugged in.   Finally, a more general study of the properties of the structured coherence measure would be useful. 



\newpage 

\footnotesize

# References {-}

